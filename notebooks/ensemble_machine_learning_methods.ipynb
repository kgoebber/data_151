{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6596551-a8d7-4b81-a62a-fab7c0c86502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if working in Google CoLab\n",
    "!pip uninstall scikit-learn -y\n",
    "!pip uninstall imbalanced-learn -y\n",
    "\n",
    "!pip install -U scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed5daa-308c-46ed-b822-b4e70671e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingRegressor, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb32bab-e760-4682-a19c-cddc43cd8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(ytrue, y_pred):\n",
    "    '''Definition for computing and printing a series of Classification metric scores'''\n",
    "    print(f'Accuracy Score: {metrics.accuracy_score(ytrue, y_pred)}')\n",
    "    print(f'Precision Score: {metrics.precision_score(ytrue, y_pred, average=\"macro\")}')\n",
    "    print(f'Recall Score: {metrics.recall_score(ytrue, y_pred, average=\"macro\")}')\n",
    "    print(f'F1 Score: {metrics.f1_score(ytrue, y_pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98bbdd-52c5-48d6-8a7a-8fcea1ad2123",
   "metadata": {},
   "source": [
    "## Red Wine Dataset\n",
    "\n",
    "We'll read in our red wine dataset and add three random variables to help us choose the most relevant parameters to use for our prediction.\n",
    "\n",
    "The random values that we'll add are:\n",
    "* random binary (e.g., 0 and 1)\n",
    "* random uniform (e.g., float values between 0 and 1)\n",
    "* random integers (e.g., random integer values between -1000 and 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41191461-3b43-47c2-a642-0b346bc9abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "\n",
    "np.random.seed(1000)\n",
    "df['rand_binary'] = np.random.randint(2, size=df.shape[0])\n",
    "df['rand_uniform'] = np.random.random_sample(size=df.shape[0])\n",
    "df['rand_integer'] = np.random.randint(-1000, 1001, size=df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3826b-f924-421f-97f1-1072e4fefaa7",
   "metadata": {},
   "source": [
    "## Correlation of Features to Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c6471-4613-4c4b-8d28-da1f5ecdb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(df.corr(), annot=True, cmap=plt.cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b2c21-b248-4bf8-b8f5-c4cbec6d42f3",
   "metadata": {},
   "source": [
    "We could just pick those feature variables that are more highly correlated than our random variables to reduce the total number of features and attempt to avoid overfitting our model.\n",
    "\n",
    "Or we can use all of our variables, including the random variables and have an initial model tell us about the importance of each variable and only choose those that are of greater importance than any of our random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25daeab8-83ea-4f91-96aa-cf8401106a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all of our features\n",
    "features = df.drop(columns=['quality'])\n",
    "target = df['quality']\n",
    "\n",
    "# Split our data into training and testing datasets\n",
    "xtrain, xtest, ytrain, ytrue = train_test_split(features, target, random_state=2)\n",
    "\n",
    "# Oversample our imbalanced data\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(xtrain, ytrain)\n",
    "\n",
    "# Use a decision tree to create an initial model\n",
    "model = DecisionTreeClassifier(random_state=4)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# List the important features\n",
    "important_index = np.argsort(model.feature_importances_)[::-1]\n",
    "for index in important_index:\n",
    "    print(f'{model.feature_names_in_[index]}: {model.feature_importances_[index]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1bfee6-9b9c-4161-888f-2990cf371ae8",
   "metadata": {},
   "source": [
    "### Create Base Model\n",
    "Use only the most important features and create a simple decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118386d-8172-42fb-aa6b-e790b5100c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[model.feature_names_in_[important_index[:7]]]\n",
    "target = df['quality']\n",
    "\n",
    "xtrain, xtest, ytrain, ytrue = train_test_split(features, target, random_state=2)\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(xtrain, ytrain)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=4)\n",
    "\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(xtest)\n",
    "\n",
    "model_validation(ytrue, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc2585-b442-4c9a-baaf-bae499f3e3a1",
   "metadata": {},
   "source": [
    "Now let's make a pipeline to scale our data to not make big value features more important and see if that improves our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292125ec-5c3f-4c3a-aa4b-85a4d50b9b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=4))\n",
    "\n",
    "pipeline.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = pipeline.predict(xtest)\n",
    "\n",
    "model_validation(ytrue, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eeaff5-9bcd-4da5-b7d2-348c761076ba",
   "metadata": {},
   "source": [
    "## Random Forecast (Ensemble - Bagging)\n",
    "\n",
    "Now let's implement an ensemble technique (bagging) by creating a Random Forecast model. We'll set the max number of features based on the number being used in our prediction and continuing to use the oversampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c3c9d-b1fe-49da-bb65-0ad3a8337cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter for max number of features for Random Forest\n",
    "m = np.int64(np.sqrt(features.shape[1]))\n",
    "\n",
    "# Set up our Random Forest\n",
    "random_forest = RandomForestClassifier(max_features=m, max_samples=0.75, oob_score=True, random_state=4)\n",
    "random_forest.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = random_forest.predict(xtest)\n",
    "\n",
    "model_validation(ytrue, y_pred)\n",
    "\n",
    "random_forest.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83072592-c265-4dd0-862e-c941957a82de",
   "metadata": {},
   "source": [
    "What about it we weighted our features instead of using the over sampled data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d872b5b-3e4f-4ac4-9a3c-116aba2585f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(ytrain)\n",
    "cw = class_weight.compute_class_weight('balanced', classes=classes, y=ytrain)\n",
    "weights = dict(zip(classes, cw))\n",
    "\n",
    "random_forest = RandomForestClassifier(max_features=m, class_weight=weights)\n",
    "random_forest.fit(xtrain, ytrain)\n",
    "\n",
    "y_pred = random_forest.predict(xtest)\n",
    "\n",
    "model_validation(ytrue, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862015a1-92ec-441a-b1a9-4b8d52c968e3",
   "metadata": {},
   "source": [
    "## Random Forecast - Boosting\n",
    "\n",
    "Now let's try a different ensemble technique using the AdaBoost method to use weak learners to enhance the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a0a9c-df85-4d1d-979d-96b7fedaaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_features=m, max_samples=.75, random_state=4),\n",
    "                              learning_rate=.1, n_estimators=25, random_state=1)\n",
    "\n",
    "adaboost.fit(xtrain, ytrain)\n",
    "\n",
    "y_pred = adaboost.predict(xtest)\n",
    "\n",
    "model_validation(ytrue, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3cbb7-bba3-4309-9c40-3bdeb4cfd008",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Prediction with Gradient Boosting\n",
    "\n",
    "Let's try our hand at another type of prediction using our highly imbalanced credit card fraud data set and using a Boosting technique.\n",
    "\n",
    "Here we'll first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab42d5-10b2-4314-a05f-2e9389fa5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data set\n",
    "data = pd.read_csv('http://bergeron.valpo.edu/creditcard.csv')\n",
    "\n",
    "# normalise the amount column\n",
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    " \n",
    "# drop Time and Amount columns as they are not relevant for prediction purpose\n",
    "data = data.drop(['Time', 'Amount'], axis = 1)\n",
    "\n",
    "X = data.drop(columns=['Class'])\n",
    "y = data['Class']\n",
    "\n",
    "# split into 70:30 ration\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b1c43-1bee-49cc-88e2-17171f70b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gboost_tree = HistGradientBoostingClassifier(learning_rate=.01, random_state=0)\n",
    "\n",
    "gboost_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gboost_tree.predict(X_test)\n",
    "\n",
    "model_validation(y_test, y_pred)\n",
    "# print classification report\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c0221-2554-45b7-aeb0-56b5b9524b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(sampling_strategy='minority', random_state = 2)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "gboost_tree.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred = gboost_tree.predict(X_test)\n",
    "\n",
    "model_validation(y_test, y_pred)\n",
    "# print classification report\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d257e5-139e-4ed4-bf59-05479ecbaf12",
   "metadata": {},
   "source": [
    "## Regression - Bagging\n",
    "\n",
    "Let's take a brief look at using an ensemble technique (bagging) for a regression problem and scale our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298fc41-e17f-4fe7-bc95-a6e9da21c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_petrol = pd.read_csv('https://raw.githubusercontent.com/kgoebber/data_151/main/notebooks/petrol_consumption.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d5387-dbaf-46f8-a804-4f7b41234c9b",
   "metadata": {},
   "source": [
    "Unscaled data and the Stochastic Gradient Descent Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29f3652-a43f-45d8-b258-1b7a684030c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unscaled = df_petrol[['Petrol_tax', 'Average_income', 'Paved_Highways', 'Population_Driver_licence(%)']]\n",
    "y = df_petrol['Petrol_Consumption']\n",
    "\n",
    "X_train, X_test, y_train, ytrue = train_test_split(x_unscaled, y, test_size=0.2, random_state=1)\n",
    "\n",
    "regressor = SGDRegressor()\n",
    "regressor.fit(X_train, y_train);\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(ytrue, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(ytrue, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(ytrue, y_pred)))\n",
    "print('R^2: ', metrics.r2_score(ytrue, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f2493-d859-48e8-b66b-8c96354fa01f",
   "metadata": {},
   "source": [
    "Erorrs Abound! Need to scale our data for this type of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47541e5b-a7e3-4556-a187-73f4529007aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(df_petrol[['Petrol_tax', 'Average_income', 'Paved_Highways', 'Population_Driver_licence(%)']])\n",
    "y = df_petrol['Petrol_Consumption']\n",
    "\n",
    "X_train, X_test, y_train, ytrue = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "regressor = SGDRegressor()\n",
    "regressor.fit(X_train, y_train);\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(ytrue, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(ytrue, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(ytrue, y_pred)))\n",
    "print('R^2: ', metrics.r2_score(ytrue, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7a052-42bf-4cc2-b4aa-772539c0a976",
   "metadata": {},
   "source": [
    "Better. Now can we improve the explained variance by using a Bagging ensemble technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f450b-ea9c-42b8-9aa6-d3cebb5395bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = BaggingRegressor(base_estimator=SGDRegressor(), n_estimators=1000, max_samples=35, max_features=3)\n",
    "regressor.fit(X_train, y_train);\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(ytrue, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(ytrue, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(ytrue, y_pred)))\n",
    "print('R^2: ', metrics.r2_score(ytrue, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c9040-247a-486c-85b9-4b9fb81e905d",
   "metadata": {},
   "source": [
    "To improve our model further we would want to take a look at our features and potentially create other derived features to help our model be able to explain more variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec6390e-1445-4068-a81a-0b3758fbeb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_petrol.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d21b1-fba0-40a8-991f-85a8fccc33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_petrol, y_vars='Petrol_Consumption')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e47cea-26bf-4919-ac0d-d74bfeb2780c",
   "metadata": {},
   "source": [
    "In looking at the scatter plots it appears that the relationship of paved highways to overall petrol consumption is more complicated than a simple line, so maybe we would be able to refactor a variable in a manner that would improve that features ability to positively inform our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69d9c0-e494-4df7-ae0b-d1c96ecda6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
