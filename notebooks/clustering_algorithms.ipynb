{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f43a02-0ae6-4641-87ff-1689b267dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3adeee-bc85-447b-ae5b-60051fc61ab8",
   "metadata": {},
   "source": [
    "## Customer Segmentation\n",
    "\n",
    "Let's read in a new dataset that gives information about spending habits at a mall. We want to learn about the relationships between these customers to see if we can identify common groupings for some future marketing efforts. To be efficient in our efforts we want to better align our communication and offers with those most likely to spend money at the mall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc060de1-ba4d-4613-a33b-67bfb4d9a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/kgoebber/data_151/main/notebooks/Mall_Customers.csv\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b0d6a-844e-4361-9f93-a272eca1d2d9",
   "metadata": {},
   "source": [
    "Let's begin by looking at the correlation of our variables, including a binary gender category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20901ce-913b-4920-ac9f-f066d455e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender_Binary'] = LabelBinarizer().fit_transform(df['Gender'])\n",
    "df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)', 'Gender_Binary']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73709a-6e5f-4500-b9e6-f369eff58935",
   "metadata": {},
   "source": [
    "It appears that our two main related variables are Age and Spending Score, so let's take a look at the joint distribution of two of our variables to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6511f-03a1-457a-8371-98399bf0de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.scatter(df[\"Age\"], df[\"Spending Score (1-100)\"])\n",
    "\n",
    "ax.set_title(\"Clusters Identified by kMeans Model\")    \n",
    "ax.set_ylabel(\"Spending Score (1-100)\")\n",
    "ax.set_xlabel(\"Age\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601d58a-9adf-42f1-bc18-37043187ec2e",
   "metadata": {},
   "source": [
    "## k-means clustering\n",
    "\n",
    "Let's use a simple clustering scheme to see if we can identify some common groupings. In order to use k-means clustering we need to know how many clusters we want to find. Looking at our image above, it is not clearly evident how many we should choose and we have no other information that would allow us to analytically choose the number of clusters. So let's make a series of clusters and use the intertia measure (intertia is the sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided) as a way to identify and optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f23b5-31cc-4fc4-9a53-ff7a616d47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Age', 'Spending Score (1-100)']]\n",
    "\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8f29d-91c1-4388-832e-1797418aeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(range(1, 11), wcss)\n",
    "ax.set_title('Selecting the Numbeer of Clusters using the Elbow Method')\n",
    "ax.set_xlabel('Clusters')\n",
    "ax.set_ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38441f-9686-4880-834c-db19da577f0a",
   "metadata": {},
   "source": [
    "From the above graphic we can identify where there are sharp \"elbows\" indicating a change in the slope of the line. We can idenify the number of clusters to use to look for the last elbow as we increase the number of clusters. For our data, it appears that 4 would be a good number to choose for our segmentation efforts.\n",
    "\n",
    "Now lets do a k-means cluster with four clusters and plot our data to identify the different clusters by color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca25192-4283-4ae7-a27b-e0cd879f13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Age', 'Spending Score (1-100)']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "cluster_labels = kmeans.predict(X)\n",
    "X = pd.DataFrame(X)\n",
    "X['cluster'] = cluster_labels\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111)\n",
    "for k in range(0, 4):\n",
    "    data = X[X[\"cluster\"]==k]\n",
    "    ax.scatter(data[\"Age\"], data[\"Spending Score (1-100)\"], label=k)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"Clusters Identified by kMeans Model\")    \n",
    "ax.set_ylabel(\"Spending Score (1-100)\")\n",
    "ax.set_xlabel(\"Age\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc53f73-65e5-4694-86dc-1dba89c11802",
   "metadata": {},
   "source": [
    "How do we interpret these clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4630a72-23f8-42a2-b3a6-f9ed949bed8e",
   "metadata": {},
   "source": [
    "## Gaussian Mixture\n",
    "\n",
    "Let's now do the same thing with a different clusering method. We'll also describe a few characteristics of each cluster to help us interpret our clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c1484-f7af-4936-b643-5cfeae86a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Age', 'Spending Score (1-100)']]\n",
    "\n",
    "n_clusters = 4\n",
    "gmm_model = GaussianMixture(n_components=n_clusters, random_state=100)\n",
    "gmm_model.fit(X)\n",
    "\n",
    "cluster_labels = gmm_model.predict(X)\n",
    "X = pd.DataFrame(X)\n",
    "X['cluster'] = cluster_labels\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for k in range(0, n_clusters):\n",
    "    data = X[X[\"cluster\"]==k]\n",
    "    ax.scatter(data[\"Age\"], data[\"Spending Score (1-100)\"], label=k)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title(\"Clusters Identified by Guassian Mixture Model\")    \n",
    "ax.set_ylabel(\"Spending Score (1-100)\")\n",
    "ax.set_xlabel(\"Age\")\n",
    "plt.show()\n",
    "\n",
    "for group in np.sort(X.cluster.unique()):\n",
    "    print(f'Group {group}')\n",
    "    print(f' Avg. Age: {X.Age[X.cluster == group].mean()}')\n",
    "    print(f\" Avg. Spend Score: {X['Spending Score (1-100)'][X.cluster == group].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26026253-9d4d-4175-b1fa-bd7fb0192c9d",
   "metadata": {},
   "source": [
    "## Spectral Clustering\n",
    "Let's compare our previous clustering to using a spectral clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c67c98-04e7-484d-868b-77f7b21ac70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Age', 'Spending Score (1-100)']]\n",
    "\n",
    "n_clusters = 4\n",
    "\n",
    "spectral_cluster_model= SpectralClustering(\n",
    "    n_clusters=n_clusters, \n",
    "    random_state=25, \n",
    "    n_neighbors=8, \n",
    "    affinity='nearest_neighbors'\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "X['cluster'] = spectral_cluster_model.fit_predict(X[['Age', 'Spending Score (1-100)']])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111)\n",
    "for k in range(0, n_clusters):\n",
    "    data = X[X[\"cluster\"]==k]\n",
    "    ax.scatter(data[\"Age\"], data[\"Spending Score (1-100)\"], label=k)\n",
    "\n",
    "ax.legend()\n",
    "ax.set(title='Spectral Clustering')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9ccd0-116c-46b7-818e-5df61c79b819",
   "metadata": {},
   "source": [
    "## Clustering with Classification Prediction\n",
    "So clusering by itself is useful, but how about combining clustering with a supervised learning algorithm to predict something. Let's go back to our wine data and perform some clustering to identify an unsupervised category for each observation and add that to our prediction scheme.\n",
    "\n",
    "We can use the elbow method to identify an appropriate number of clusters or in this case we know we have a unique set of categories that could serve as the different number of clusters. First, let's see what the \"elbow\" analysis gives us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55cf1c-6cec-4a56-a2e1-c57fb4cd6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "\n",
    "X = df.drop(columns='quality')\n",
    "\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.plot(range(1, 11), wcss)\n",
    "\n",
    "ax.set_title('Selecting the Numbeer of Clusters using the Elbow Method')\n",
    "ax.set_xlabel('Clusters')\n",
    "ax.set_ylabel('WCSS')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f6157-8971-4ade-87bf-9d2467431ef6",
   "metadata": {},
   "source": [
    "So again, it appears four might be a good number to start with - which is not too dissimilar from the number of categories we have in the dataset (6; 3, 4, 5, 6, 7, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6ea22-7231-41c1-9944-b8d1c15ab671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validation(ytrue, y_pred):\n",
    "    '''Definition for computing and printing a series of Classification metric scores'''\n",
    "    print(f'Accuracy Score: {metrics.accuracy_score(ytrue, y_pred)}')\n",
    "    print(f'Precision Score: {metrics.precision_score(ytrue, y_pred, average=\"macro\")}')\n",
    "    print(f'Recall Score: {metrics.recall_score(ytrue, y_pred, average=\"macro\")}')\n",
    "    print(f'F1 Score: {metrics.f1_score(ytrue, y_pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd847dc6-fc0f-4d7e-a40f-d3e8eea8a6ac",
   "metadata": {},
   "source": [
    "Okay, now that we know how many clusters we'll use, let's begin by gathering our best predictor variables, performing the clustering with our feature variables, adding those new clusters to the feature matrix, then doing our prediction with the extra predictors using oversampling with bagging and boosting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e36b50-5137-4ccb-bfed-4fe4eb2dcbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['volatile acidity', 'citric acid', 'sulphates', 'alcohol',\n",
    "        'chlorides', 'total sulfur dioxide', 'fixed acidity']]\n",
    "y = df['quality']\n",
    "\n",
    "n_clusters = len(y.unique())\n",
    "gmm_model = GaussianMixture(n_components=n_clusters, random_state=100)\n",
    "gmm_model.fit(X)\n",
    "\n",
    "X = pd.DataFrame(X)\n",
    "cluster_labels = gmm_model.predict(X)\n",
    "\n",
    "# Option 1: simply add the cluster label as a category\n",
    "# X['cluster'] = cluster_labels\n",
    "\n",
    "# Option 2: Use one hot encoding to add a feature vector to the DataFrame\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "cluster_ohe = ohe.fit_transform(cluster_labels.reshape(-1, 1))\n",
    "ohe_df = pd.DataFrame(cluster_ohe, columns=np.unique(cluster_labels))\n",
    "X.append(ohe_df)\n",
    "\n",
    "# Split our data into training and testing datasets\n",
    "xtrain, xtest, ytrain, ytrue = train_test_split(X, y, random_state=2)\n",
    "\n",
    "# Oversample our features\n",
    "ros = RandomOverSampler()\n",
    "X_resampled, y_resampled = ros.fit_resample(xtrain, ytrain)\n",
    "\n",
    "# Set parameter for max number of features for Random Forest\n",
    "m = np.int64(np.sqrt(X.shape[1]))\n",
    "\n",
    "# Use a Random Forecast and Boosting Method for prediction\n",
    "adaboost = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_features=m, max_samples=.75, random_state=4),\n",
    "                              learning_rate=.1, n_estimators=25, random_state=1)\n",
    "\n",
    "adaboost.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = adaboost.predict(xtest)\n",
    "\n",
    "model_validation(ytrue, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217cd61f-29dd-480b-aaf8-07a4a7e983dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
